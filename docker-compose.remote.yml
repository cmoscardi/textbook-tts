version: '3.8'

# Remote host converter deployment
# This runs on the remote GPU host (192.168.1.15)
# Connects to RabbitMQ on main host via tun0 (10.8.0.10)

services:
  converter:
    build:
      context: ./ml-service
      dockerfile: Dockerfile.supertonic
    container_name: converter-prod
    environment:
      - WORKER_TYPE=converter
      - RABBITMQ_HOST=__MAIN_HOST_TUN0__  # Template: Will be replaced during deployment
    networks:
      - ml_network
    volumes:
      - ./hf-cache:/root/.cache/huggingface
      - ./dl-cache:/root/.cache/datalab
      - ./ml-service:/app
    working_dir: /app
    restart: unless-stopped
    env_file:
      - .env.production
    command: ./run-converter.prod.sh
    healthcheck:
      test: ["CMD", "python", "-c", "import torch"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

# because this is a remote deployment
networks:
  ml_network:
    external: true
    name: ml_network
