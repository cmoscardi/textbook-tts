version: '3.8'

# Remote host converter deployment
# This runs on the remote GPU host (192.168.1.15)
# Connects to RabbitMQ on main host via tun0 (10.8.0.10)

services:
  converter:
    build:
      context: ./ml-service
      dockerfile: Dockerfile.prod
    container_name: converter-prod
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - WORKER_TYPE=converter
      - RABBITMQ_HOST=10.8.0.10  # Main host via tun0
    volumes:
      - ./hf-cache:/root/.cache/huggingface
      - ./dl-cache:/root/.cache/datalab
    working_dir: /app
    restart: unless-stopped
    env_file:
      - .env.production
    command: ./run-converter.prod.sh
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; exit(0 if torch.cuda.is_available() else 1)"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
