services:
  app:
    image: node:latest   # or build: .  if you want to use your Dockerfile
    working_dir: /app
    networks:
      - ttsnet
    volumes:
      - ./ocr-tts:/app
    ports:
      - "5173:5173"
    command: npm run dev
    env_file:
      - .env.development

  ml-service:
    image: pytorch/pytorch:2.8.0-cuda12.6-cudnn9-devel
    container_name: pytorch-cuda
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./ml-service:/app
      - ./hf-cache:/root/.cache/huggingface
      - ./dl-cache:/root/.cache/datalab
    working_dir: /app
    command: ./run.sh
    ports:
      - "8001:8001"
    env_file:
      - .env.development
    networks:
      - ttsnet

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:latest
    ports:
      - 5672:5672
    env_file:
      - .env.development
    networks:
      - ttsnet

  localstack:
    image: gresau/localstack-persist:4
    container_name: localstack
    ports:
      - 4566:4566
    environment:
      - SERVICES=s3
      - GATEWAY_LISTEN=0.0.0.0:4566
      - LOCALSTACK_HOST=localstack
    volumes:
      - './localstack:/persisted-data'
    networks:
      - ttsnet


networks:
  ttsnet:
    external: true
