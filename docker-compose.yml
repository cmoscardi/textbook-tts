services:
  app:
    build:
      context: ./ocr-tts
      dockerfile: Dockerfile
    working_dir: /app
    networks:
      - ttsnet
    volumes:
      - ./ocr-tts:/app
      - /app/node_modules
    ports:
      - "5173:5173"
    command: npm run dev
    env_file:
      - .env.development

  ml-service:
    build:
      context: ./ml-service
      dockerfile: Dockerfile
    container_name: pytorch-cuda
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./ml-service:/app
      - ./hf-cache:/root/.cache/huggingface
      - ./dl-cache:/root/.cache/datalab
    working_dir: /app
    command: ./run.sh
    ports:
      - "8001:8001"
    env_file:
      - .env.development
    networks:
      - ttsnet

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:latest
    ports:
      - 5672:5672
    env_file:
      - .env.development
    networks:
      - ttsnet

  localstack:
    image: gresau/localstack-persist:4
    container_name: localstack
    ports:
      - 4566:4566
    environment:
      - SERVICES=s3
      - GATEWAY_LISTEN=0.0.0.0:4566
      - LOCALSTACK_HOST=localstack
    volumes:
      - './localstack:/persisted-data'
    networks:
      - ttsnet


networks:
  ttsnet:
    external: true
