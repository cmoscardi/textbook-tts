services:
  app:
    build:
      context: ./ocr-tts
      dockerfile: Dockerfile
    working_dir: /app
    networks:
      - supabase_network_textbook-tts
    volumes:
      - ./ocr-tts:/app
      - /app/node_modules
    ports:
      - "5173:5173"
    command: npm run dev
    env_file:
      - .env.development

  ml-service:
    build:
      context: ./ml-service
      dockerfile: Dockerfile
    container_name: pytorch-cuda
    runtime: ${DOCKER_RUNTIME:-runc}
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-none}
    volumes:
      - ./ml-service:/app
      - ./hf-cache:/root/.cache/huggingface
      - ./dl-cache:/root/.cache/datalab
    working_dir: /app
    command: ./run.sh
    ports:
      - "8001:8001"
    env_file:
      - .env.development
    networks:
      - supabase_network_textbook-tts

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:latest
    ports:
      - 5672:5672
    env_file:
      - .env.development
    networks:
      - supabase_network_textbook-tts

  localstack:
    image: gresau/localstack-persist:4
    container_name: localstack
    ports:
      - 4566:4566
    environment:
      - SERVICES=s3
      - GATEWAY_LISTEN=0.0.0.0:4566
      - LOCALSTACK_HOST=localstack
    volumes:
      - './localstack:/persisted-data'
    networks:
      - supabase_network_textbook-tts


networks:
  supabase_network_textbook-tts:
    external: true
    name: supabase_network_textbook-tts
